https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.8.tar.gz


shell$ tar xf openmpi-<version>.tar.bz2
shell$ cd openmpi-<version>
shell$ ./configure --prefix=<path> [...options...] 2>&1 | tee config.out
<... lots of output ...>

# Use an integer value of N for parallel builds
shell$ make [-j N] all 2>&1 | tee make.out



user with root permissions


import paramiko

# List of your target machines
hosts = ['192.168.1.101', '192.168.1.102', 'server.example.com']
username = 'your_existing_user'
password = 'your_existing_password' # A temporary password or existing user credentials

# The new user to be created and their password
new_admin_user = 'new_admin'
new_admin_password = 'a_secure_password'

# SSH public key content to be added
ssh_public_key = 'ssh-rsa AAAAB3Nz...' # Replace with your actual public key

for host in hosts:
    try:
        # Establish an SSH connection
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        ssh.connect(host, username=username, password=password)



*********************************************************************************************************************************


Create a hostfile (e.g., my_hosts.txt):
code
Code
# This is my compute cluster
# Lines starting with # are comments

# The first machine
node01.mycluster.local

# The second machine. The 'slots' keyword specifies
# how many processes can be run on this machine.
node02.mycluster.local slots=8

# The third machine has 2 CPUs with 4 cores each
node03.mycluster.local slots=8
slots is a key concept. It tells MPI the capacity of a machine, usually corresponding to the number of CPU cores. If slots is not specified, it often defaults to 1.
Tell mpirun to use the hostfile:
code
Bash

# Run 16 processes across the machines listed in my_hosts.txt,

# respecting the 'slots' for each.

mpirun -n 16 --hostfile my_hosts.txt ./my_mpi_app

How it works: mpirun reads and parses my_hosts.txt. It sees it has a total capacity of 1 (default) + 8 + 8 = 17 slots. You requested 16 processes, so it will distribute them: 1 process on node01, 8 on node02, and the remaining 7 on node03. Then, it ssh's to node01, node02, and node03 to launch the daemons and your application.
Method 3: Integration with a Job Scheduler (The Professional HPC Approach)
In any real High-Performance Computing (HPC) center, you don't run mpirun directly on a list of hosts you picked yourself. You submit your job to a resource manager or job scheduler like Slurm, PBS/Torque, or LSF.
This is the most sophisticated and robust method.
You Request Resources: You write a submission script that tells the scheduler what you need.

Example Slurm submission script (submit.sh):

code

Bash

#!/bin/bash
#SBATCH --nodes=4          # Request 4 entire machines
#SBATCH --ntasks-per-node=32 # Request 32 processes on each machine
#SBATCH --job-name=my_sim

# The scheduler will handle finding the hosts.
# We just use mpirun directly.

mpirun ./my_mpi_app

The Scheduler Allocates Machines: The scheduler looks at the state of the entire cluster, finds 4 idle nodes that meet your requirements, and reserves them for your job.
The Scheduler Informs MPI: This is the magic step. When the scheduler starts your script on the first of your allocated nodes, it sets special environment variables. mpirun is built to automatically detect these variables.

Slurm creates a variable like SLURM_NODELIST.
PBS/Torque creates a file pointed to by $PBS_NODEFILE.
mpirun Reads the Environment: When you simply type mpirun ./my_mpi_app inside the job script, the MPI runtime environment automatically:
Detects it's running inside a Slurm (or PBS, etc.) job.

Reads the SLURM_NODELIST environment variable to get the list of hosts the scheduler assigned.

Parses that list.

ssh's to those specific hosts to launch the job.
You never have to manually create a hostfile or type hostnames. The multi-million dollar scheduler does the "discovery" (allocation) for you, and mpirun simply listens to what the scheduler tells it.

Summary: The Party Invitation Analogy
Think of it like planning a party (./my_mpi_app).
Method 1 (Command Line): You stand at the door and shout the names of the two friends you want to let in.
Method 2 (Hostfile): You write down a guest list (my_hosts.txt) and give it to your bouncer (mpirun), who then only lets in people on the list.
Method 3 (Job Scheduler): You hire a professional event planning company (Slurm). They find a venue, book it, and give your bouncer (mpirun) the official, approved guest list for that night. The bouncer just has to read the list they are given.

In all cases, MPI is not wandering the streets looking for guests; it works from a pre-defined, explicit list of machines.

        # Create the new user and set their password
        ssh.exec_command(f'sudo useradd -m -s /bin/bash {new_admin_user}')
        ssh.exec_command(f'echo "{new_admin_user}:{new_admin_password}" | sudo chpasswd')

        # Add the new user to the sudo group (or wheel group, depending on the distro)
        ssh.exec_command(f'sudo usermod -aG sudo {new_admin_user}')

        # Set up passwordless SSH login for the new user
        ssh.exec_command(f'sudo su - {new_admin_user} -c "mkdir -p .ssh && echo \'{ssh_public_key}\' >> .ssh/authorized_keys && chmod 700 .ssh && chmod 600 .ssh/authorized_keys"')

        print(f"✅ Successfully set up {new_admin_user} on {host}")
        ssh.close()
    except Exception as e:
        print(f"❌ Failed to connect or setup on {host}: {e}")

 Running Remote Scripts
Once the new admin account is set up with SSH key-based authentication, you can use the same Python script with Paramiko or Fabric to run any remote script.

Your script would:

Connect to each machine using the new admin user and your private key.

Use ssh.exec_command() to execute the desired script or commands.

For more complex tasks, you can use sftp (Secure File Transfer Protocol) to upload the script file and then execute it remotely.

*****************************************************************


Method 2: The Hostfile (The Most Common Manual Approach)


For anything more than two or three machines, using the command line becomes cumbersome. The standard approach is to use a hostfile. This is a simple text file that lists the machines you want to use.

Create a hostfile (e.g., my_hosts.txt):

# This is my compute cluster
# Lines starting with # are comments

# The first machine
node01.mycluster.local

# The second machine. The 'slots' keyword specifies
# how many processes can be run on this machine.
node02.mycluster.local slots=8

# The third machine has 2 CPUs with 4 cores each
node03.mycluster.local slots=8
slots is a key concept. It tells MPI the capacity of a machine, usually corresponding to the number of CPU cores. If slots is not specified, it often defaults to 1.
Tell mpirun to use the hostfile:

# Run 16 processes across the machines listed in my_hosts.txt,
# respecting the 'slots' for each.

mpirun -n 16 --hostfile my_hosts.txt ./my_mpi_app

How it works: mpirun reads and parses my_hosts.txt. It sees it has a total capacity of 1 (default) + 8 + 8 = 17 slots. You requested 16 processes, so it will distribute them: 1 process on node01, 8 on node02, and the remaining 7 on node03. Then, it ssh's to node01, node02, and node03 to launch the daemons and your application.

Method 3: Integration with a Job Scheduler (The Professional HPC Approach)
In any real High-Performance Computing (HPC) center, you don't run mpirun directly on a list of hosts you picked yourself. You submit your job to a resource manager or job scheduler like Slurm, PBS/Torque, or LSF.
This is the most sophisticated and robust method.
You Request Resources: You write a submission script that tells the scheduler what you need.

Example Slurm submission script (submit.sh):

#!/bin/bash
#SBATCH --nodes=4          # Request 4 entire machines
#SBATCH --ntasks-per-node=32 # Request 32 processes on each machine
#SBATCH --job-name=my_sim

# The scheduler will handle finding the hosts.
# We just use mpirun directly.
mpirun ./my_mpi_app
The Scheduler Allocates Machines: The scheduler looks at the state of the entire cluster, finds 4 idle nodes that meet your requirements, and reserves them for your job.
The Scheduler Informs MPI: This is the magic step. When the scheduler starts your script on the first of your allocated nodes, it sets special environment variables. mpirun is built to automatically detect these variables.

Slurm creates a variable like SLURM_NODELIST.
PBS/Torque creates a file pointed to by $PBS_NODEFILE.
mpirun Reads the Environment: When you simply type mpirun ./my_mpi_app inside the job script, the MPI runtime environment automatically:
Detects it's running inside a Slurm (or PBS, etc.) job.
Reads the SLURM_NODELIST environment variable to get the list of hosts the scheduler assigned.

Parses that list.
ssh's to those specific hosts to launch the job.

You never have to manually create a hostfile or type hostnames. The multi-million dollar scheduler does the "discovery" (allocation) for you, and mpirun simply listens to what the scheduler tells it.
Summary: The Party Invitation Analogy
Think of it like planning a party (./my_mpi_app).
Method 1 (Command Line): You stand at the door and shout the names of the two friends you want to let in.
Method 2 (Hostfile): You write down a guest list (my_hosts.txt) and give it to your bouncer (mpirun), who then only lets in people on the list.
Method 3 (Job Scheduler): You hire a professional event planning company (Slurm). They find a venue, book it, and give your bouncer (mpirun) the official, approved guest list for that night. The bouncer just has to read the list they are given.
In all cases, MPI is not wandering the streets looking for guests; it works from a pre-defined, explicit list of machines.
32.2s



